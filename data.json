[
  {
    "Title": "Conditional generative models for sampling and phase transition indication in spin systems, by Japneet Singh, Mathias S. Scheurer, Vipul Arora\n",
    "Link": "https://scipost.org/SciPostPhys.11.2.043",
    "Text": "In this work, we study generative adversarial networks (GANs) as a tool to learn the distribution of spin configurations and to generate samples, conditioned on external tuning parameters or other quantities associated with individual configurations. For concreteness, we focus on two examples of conditional variables---the temperature of the system and the energy of the samples. We show that temperature-conditioned models can not only be used to generate samples across thermal phase transitions, but also be employed as unsupervised indicators of transitions. To this end, we introduce a GAN-fidelity measure that captures the model\u2019s susceptibility to external changes of parameters. The proposed energy-conditioned models are integrated with Monte Carlo simulations to perform over-relaxation steps, which break the Markov chain and reduce auto-correlations. We propose ways of efficiently representing the physical states in our network architectures, e.g., by exploiting symmetries, and to minimize the correlations between generated samples. A detailed evaluation, using the two-dimensional XY model as an example, shows that these incorporations bring in considerable improvements over standard machine-learning approaches. We further study the performance of our architectures when no training data is provided near the critical region."
  },
  {
    "Title": "Symmetry meets AI, by Gabriela Barenboim, Johannes Hirn, Veronica Sanz\n",
    "Link": "https://scipost.org/SciPostPhys.11.1.014",
    "Text": "We explore whether Neural Networks (NNs) can {\\it discover} the presence of symmetries as they learn to perform a task. For this, we train hundreds of NNs on a {\\it decoy task} based on well-controlled Physics templates, where no information on symmetry is provided. We use the output from the last hidden layer of all these NNs, projected to fewer dimensions, as the input for a symmetry classification task, and show that information on symmetry had indeed been identified by the original NN without guidance. As an interdisciplinary application of this procedure, we identify the presence and level of symmetry in artistic paintings from different styles such as those of Picasso, Pollock and Van Gogh."
  },
  {
    "Title": "Machine learning and quantum devices, by Florian Marquardt\n",
    "Link": "https://scipost.org/SciPostPhysLectNotes.29",
    "Text": "These brief lecture notes cover the basics of neural networks and deep learning as well as their applications in the quantum domain, for physicists without prior knowledge. In the first part, we describe training using backpropagation, image classification, convolutional networks and autoencoders. The second part is about advanced techniques like reinforcement learning (for discovering control strategies), recurrent neural networks (for analyzing time traces), and Boltzmann machines (for learning probability distributions). In the third lecture, we discuss first recent applications to quantum physics, with an emphasis on quantum information processing machines. Finally, the fourth lecture is devoted to the promise of using quantum effects to accelerate machine learning."
  },
  {
    "Title": "Supervised learning of few dirty bosons with variable particle number, by Pere Mujal, \u00c0lex Mart\u00ednez Miguel, Artur Polls, Bruno Juli\u00e1-D\u00edaz, Sebastiano Pilati\n",
    "Link": "https://scipost.org/SciPostPhys.10.3.073",
    "Text": "We investigate the supervised machine learning of few interacting bosons in optical speckle disorder via artificial neural networks. The learning curve shows an approximately universal power-law scaling for different particle numbers and for different interaction strengths. We introduce a network architecture that can be trained and tested on heterogeneous datasets including different particle numbers. This network provides accurate predictions for all system sizes included in the training set and, by design, is suitable to attempt extrapolations to (computationally challenging) larger sizes. Notably, a novel transfer-learning strategy is implemented, whereby the learning of the larger systems is substantially accelerated and made consistently accurate by including in the training set many small-size instances."
  },
  {
    "Title": "Improved neural network Monte Carlo simulation, by I-Kai Chen, Matthew D. Klimek, Maxim Perelstein\n",
    "Link": "https://scipost.org/SciPostPhys.10.1.023",
    "Text": "The algorithm for Monte Carlo simulation of parton-level events based on an Artificial Neural Network (ANN) proposed in arXiv:1810.11509 is used to perform a simulation of $H\\to 4\\ell$ decay. Improvements in the training algorithm have been implemented to avoid numerical instabilities. The integrated decay width evaluated by the ANN is within 0.7% of the true value and unweighting efficiency of 26% is reached. While the ANN is not automatically bijective between input and output spaces, which can lead to issues with simulation quality, we argue that the training procedure naturally prefers bijective maps, and demonstrate that the trained ANN is bijective to a very good approximation."
  },
  {
    "Title": "Neural network-based approach to phase space integration, by Matthew D. Klimek, Maxim Perelstein\n",
    "Link": "https://scipost.org/SciPostPhys.9.4.053",
    "Text": "Monte Carlo methods are widely used in particle physics to integrate and sample probability distributions (differential cross sections or decay rates) on multi-dimensional phase spaces. We present a Neural Network (NN) algorithm optimized to perform this task. The algorithm has been applied to several examples of direct relevance for particle physics, including situations with non-trivial features such as sharp resonances and soft/collinear enhancements. Excellent performance has been demonstrated in all examples, with the properly trained NN achieving unweighting efficiencies of between 30% and 75%. In contrast to traditional Monte Carlo algorithms such as VEGAS, the NN-based approach does not require that the phase space coordinates be aligned with resonant or other features in the cross section."
  },
  {
    "Title": "Detecting nematic order in STM/STS data with artificial intelligence, by Jeremy B. Goetz, Yi Zhang, Michael J. Lawler\n",
    "Link": "https://scipost.org/SciPostPhys.8.6.087",
    "Text": "Detecting the subtle yet phase defining features in Scanning Tunneling Microscopy and Spectroscopy data remains an important challenge in quantum materials. We meet the challenge of detecting nematic order from local density of states data with supervised machine learning and artificial neural networks for the difficult scenario without sharp features such as visible lattice Bragg peaks or Friedel oscillation signatures in the Fourier transform spectrum. We train the artificial neural networks to classify simulated data of isotropic and anisotropic two-dimensional metals in the presence of disorder. The supervised machine learning succeeds only with at least one hidden layer in the ANN architecture, demonstrating it is a higher level of complexity than nematic order detected from Bragg peaks which requires just two neurons. We apply the finalized ANN to experimental STM data on CaFe2As2, and it predicts nematic symmetry breaking with 99% confidence (probability 0.99), in agreement with previous analysis. Our results suggest ANNs could be a useful tool for the detection of nematic order in STM data and a variety of other forms of symmetry breaking."
  },
  {
    "Title": "From complex to simple : hierarchical free-energy landscape renormalized in deep neural networks, by Hajime Yoshino\n",
    "Link": "https://scipost.org/SciPostPhysCore.2.2.005",
    "Text": "We develop a statistical mechanical approach based on the replica method to study the design space of deep and wide neural networks constrained to meet a large number of training data. Specifically, we analyze the configuration space of the synaptic weights and neurons in the hidden layers in a simple feed-forward perceptron network for two scenarios: a setting with random inputs/outputs and a teacher-student setting. By increasing the strength of constraints,~i.e. increasing the number of training data, successive 2nd order glass transition (random inputs/outputs) or 2nd order crystalline transition (teacher-student setting) take place layer-by-layer starting next to the inputs/outputs boundaries going deeper into the bulk with the thickness of the solid phase growing logarithmically with the data size. This implies the typical storage capacity of the network grows exponentially fast with the depth. In a deep enough network, the central part remains in the liquid phase. We argue that in systems of finite width N, the weak bias field can remain in the center and plays the role of a symmetry-breaking field that connects the opposite sides of the system. The successive glass transitions bring about a hierarchical free-energy landscape with ultrametricity, which evolves in space: it is most complex close to the boundaries but becomes renormalized into progressively simpler ones in deeper layers. These observations provide clues to understand why deep neural networks operate efficiently. Finally, we present some numerical simulations of learning which reveal spatially heterogeneous glassy dynamics truncated by a finite width $N$ effect."
  },
  {
    "Title": "Deep-learning jets with uncertainties and more, by Sven Bollweg, Manuel Haussmann, Gregor Kasieczka, Michel Luchmann, Tilman Plehn, Jennifer Thompson\n",
    "Link": "https://scipost.org/SciPostPhys.8.1.006",
    "Text": "Bayesian neural networks allow us to keep track of uncertainties, for example in top tagging, by learning a tagger output together with an error band. We illustrate the main features of Bayesian versions of established deep-learning taggers. We show how they capture statistical uncertainties from finite training samples, systematics related to the jet energy scale, and stability issues through pile-up. Altogether, Bayesian networks offer many new handles to understand and control deep learning at the LHC without introducing a visible prior effect and without compromising the network performance."
  },
  {
    "Title": "Fast counting with tensor networks, by Stefanos Kourtis, Claudio Chamon, Eduardo R. Mucciolo, Andrei E. Ruckenstein\n",
    "Link": "https://scipost.org/SciPostPhys.7.5.060",
    "Text": "We introduce tensor network contraction algorithms for counting satisfying assignments of constraint satisfaction problems (#CSPs). We represent each arbitrary #CSP formula as a tensor network, whose full contraction yields the number of satisfying assignments of that formula, and use graph theoretical methods to determine favorable orders of contraction. We employ our heuristics for the solution of #P-hard counting boolean satisfiability (#SAT) problems, namely monotone #1-in-3SAT and #Cubic-Vertex-Cover, and find that they outperform state-of-the-art solvers by a significant margin."
  }
]